{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zooyr/000/blob/master/004_jsh_03__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_u3q60di584x",
        "cellView": "form",
        "outputId": "443578e4-3459-45cf-fef0-d934b18c1792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 17 11:44:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    47W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Mounted at /content/drive\n",
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 1604, done.\u001b[K\n",
            "remote: Counting objects: 100% (303/303), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 1604 (delta 195), reused 219 (delta 138), pack-reused 1301\u001b[K\n",
            "Receiving objects: 100% (1604/1604), 3.28 MiB | 23.30 MiB/s, done.\n",
            "Resolving deltas: 100% (1005/1005), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 KB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 KB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.5/581.5 KB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 KB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 KB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for locon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lycoris_lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for elfinder-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title ## 1.1. Install Dependencies\n",
        "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.  This will also install the required libraries.\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "#root_dir\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir,\"deps\")\n",
        "repo_dir = os.path.join(root_dir,\"kohya-trainer\")\n",
        "training_dir = os.path.join(root_dir,\"LoRA\")\n",
        "pretrained_model = os.path.join(root_dir,\"pretrained_model\")\n",
        "vae_dir = os.path.join(root_dir,\"vae\")\n",
        "config_dir = os.path.join(training_dir,\"config\")\n",
        "\n",
        "#repo_dir\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir,\"tools\")\n",
        "finetune_dir = os.path.join(repo_dir,\"finetune\")\n",
        "\n",
        "for store in [\"root_dir\", \"deps_dir\", \"repo_dir\", \"training_dir\", \"pretrained_model\", \"vae_dir\", \"accelerate_config\", \"tools_dir\", \"finetune_dir\", \"config_dir\"]:\n",
        "  with capture.capture_output() as cap:\n",
        "    %store {store}\n",
        "    del cap\n",
        "\n",
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "branch = \"\" #@param {type: \"string\"}\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "mount_drive = True #@param {type: \"boolean\"}\n",
        "\n",
        "if mount_drive:\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "for dir in [deps_dir, training_dir, config_dir, pretrained_model, vae_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "  \n",
        "def clone_repo(url):\n",
        "  if not os.path.exists(repo_dir):\n",
        "    os.chdir(root_dir)\n",
        "    !git clone {url} {repo_dir}\n",
        "  else:\n",
        "    os.chdir(repo_dir)\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "\n",
        "clone_repo(repo_url)\n",
        "\n",
        "if branch:\n",
        "  os.chdir(repo_dir)\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "  with capture.capture_output() as cap:\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(name, 'r') as deps:\n",
        "      deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "    del cap \n",
        "\n",
        "def install_dependencies():\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "  if install_xformers:\n",
        "    !pip install -q --pre xformers\n",
        "    !pip install -q --pre triton\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "install_dependencies()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/drive/MyDrive/models/* /content/pretrained_model"
      ],
      "metadata": {
        "id": "Oc4r-GYI2B7_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.1. Locating Train Data Directory\n",
        "#@markdown Define the location of your training data. This cell will also create a folder based on your input. Regularization Images is `optional` and can be skipped.\n",
        "import os\n",
        "from IPython.utils import capture\n",
        "%store -r\n",
        "\n",
        "train_data_dir = \"/content/LoRA/train_data\" #@param {type:'string'}\n",
        "reg_data_dir = \"/content/LoRA/reg_data\" #@param {type:'string'}\n",
        "\n",
        "for image_dir in [train_data_dir, reg_data_dir]:\n",
        "  if image_dir:\n",
        "    with capture.capture_output() as cap:\n",
        "      os.makedirs(image_dir, exist_ok=True)\n",
        "      %store image_dir\n",
        "      del cap\n",
        "\n",
        "print(f\"Your train data directory : {train_data_dir}\")\n",
        "if reg_data_dir:\n",
        "  print(f\"Your reg data directory : {reg_data_dir}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y",
        "outputId": "ef5855ab-7c23-4e2a-99cb-e74aacf8599d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your train data directory : /content/LoRA/train_data\n",
            "Your reg data directory : /content/LoRA/reg_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Unzip Dataset\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "\n",
        "#@markdown Specify this section if your dataset is in a `zip` file and has been uploaded somewhere. This will download your dataset and automatically extract it to the `train_data_dir` if the `unzip_to` is empty. \n",
        "zipfile_url = \"/content/drive/MyDrive/train_data/train_02.zip\" #@param {'type': 'string'}\n",
        "zipfile_name = \"zipfile.zip\"\n",
        "unzip_to = \"\" #@param {'type': 'string'}\n",
        "\n",
        "hf_token = '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'\n",
        "user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "\n",
        "if unzip_to:\n",
        "  os.makedirs(unzip_to, exist_ok=True)\n",
        "else:\n",
        "  unzip_to = train_data_dir\n",
        "\n",
        "def download_dataset(url):\n",
        "  if url.startswith(\"/content\"):\n",
        "    !unzip -j -o {url} -d \"{train_data_dir}\"\n",
        "  elif url.startswith(\"https://drive.google.com\"):\n",
        "    os.chdir(root_dir)\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "\n",
        "download_dataset(zipfile_url)\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "if not zipfile_url.startswith(\"/content\"):\n",
        "  !unzip -j -o \"{root_dir}/{zipfile_name}\" -d \"{unzip_to}\"\n",
        "  os.remove(f\"{root_dir}/{zipfile_name}\")\n",
        "\n",
        "files_to_move = (\"meta_cap.json\", \\\n",
        "                 \"meta_cap_dd.json\", \\\n",
        "                 \"meta_lat.json\", \\\n",
        "                 \"meta_clean.json\")\n",
        "\n",
        "for filename in os.listdir(train_data_dir):\n",
        "  file_path = os.path.join(train_data_dir, filename)\n",
        "  if filename in files_to_move:\n",
        "    if not os.path.exists(file_path):\n",
        "      shutil.move(file_path, training_dir)\n",
        "    else: \n",
        "      os.remove(file_path)"
      ],
      "metadata": {
        "id": "eFFHVTWNZGbp",
        "cellView": "form",
        "outputId": "8d8b8f71-f353-4bf4-8928-b1416a079c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/train_data/train_02.zip\n",
            "  inflating: /content/LoRA/train_data/a01.png  \n",
            "  inflating: /content/LoRA/train_data/a02.png  \n",
            "  inflating: /content/LoRA/train_data/a03.png  \n",
            "  inflating: /content/LoRA/train_data/a04.png  \n",
            "  inflating: /content/LoRA/train_data/a05.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil \n",
        "src = \"/content/drive/MyDrive/train_data/reg.zip\"#@param {'type': 'string'}\n",
        "dst = \"/content/LoRA/reg_data\"#@param {'type': 'string'\n",
        "shutil.unpack_archive(src,dst)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5I-rWdBwWst9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Data Preprocessing"
      ],
      "metadata": {
        "id": "T-0qKyEgTchp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Training Model\n",
        "\n"
      ],
      "metadata": {
        "id": "yHNbl3O_NSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.1. Model Config\n",
        "from google.colab import drive\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "project_name = \"004_jsh_05_w\" #@param {type:\"string\"}\n",
        "if not project_name:\n",
        "  project_name = \"last\"\n",
        "pretrained_model_name_or_path = \"/content/pretrained_model/chillbasil_jockstrap.safetensors\" #@param {type:\"string\"}\n",
        "vae = \"\"  #@param {type:\"string\"}\n",
        "output_dir = \"/content/LoRA/output\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/LoRA/output` by default\n",
        "output_to_drive = True #@param {'type':'boolean'}\n",
        "\n",
        "if output_to_drive:\n",
        "  output_dir = \"/content/drive/MyDrive/LoRA/output\"\n",
        "\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')  \n",
        "\n",
        "sample_dir = os.path.join(output_dir, \"sample\")\n",
        "for dir in [output_dir, sample_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(\"Project Name: \", project_name)\n",
        "print(\"Model Version: Stable Diffusion V1.x\") if not v2 else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x\") if v2 and not v_parameterization else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x 768v\") if v2 and v_parameterization else \"\"\n",
        "print(\"Pretrained Model Path: \", pretrained_model_name_or_path) if pretrained_model_name_or_path else print(\"No Pretrained Model path specified.\")\n",
        "print(\"VAE Path: \", vae) if vae else print(\"No VAE path specified.\")\n",
        "print(\"Output Path: \", output_dir)"
      ],
      "metadata": {
        "id": "H_Q23fUEJhnC",
        "cellView": "form",
        "outputId": "5d66bb82-5e03-4c82-cf90-3e350b416245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Name:  004_jsh_05_w\n",
            "Model Version: Stable Diffusion V1.x\n",
            "Pretrained Model Path:  /content/pretrained_model/chillbasil_jockstrap.safetensors\n",
            "No VAE path specified.\n",
            "Output Path:  /content/drive/MyDrive/LoRA/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.2. Dataset Config\n",
        "import toml\n",
        "\n",
        "#@markdown ### Dreambooth Config\n",
        "train_repeats = 20 #@param {type:\"number\"}\n",
        "reg_repeats = 1 #@param {type:\"number\"}\n",
        "instance_token = \"jsh\" #@param {type:\"string\"}\t\n",
        "class_token = \"man\" #@param {type:\"string\"}\t \n",
        "#@markdown ### <br>General Config\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "flip_aug = True #@param {type:\"boolean\"}\t\n",
        "caption_extension = \".txt\" #@param [\"none\", \".txt\", \".caption\"]\t\n",
        "caption_dropout_rate = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\t\n",
        "caption_dropout_every_n_epochs = 2 #@param {type:\"number\"}\n",
        "keep_tokens = 0 #@param {type:\"number\"}\n",
        "\n",
        "config = {\n",
        "    \"general\": {\n",
        "        \"enable_bucket\": True,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"resolution\": resolution,\n",
        "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,           \n",
        "            \"caption_dropout_rate\": caption_dropout_rate if caption_extension == \".caption\" else 0,\n",
        "            \"caption_tag_dropout_rate\": caption_dropout_rate if caption_extension == \".txt\" else 0,\n",
        "            \"caption_dropout_every_n_epochs\": caption_dropout_every_n_epochs,\n",
        "            \"flip_aug\": flip_aug,\n",
        "            \"color_aug\": False,\n",
        "            \"face_crop_aug_range\": None,\n",
        "            \"subsets\": [\n",
        "                {\n",
        "                    \"image_dir\": train_data_dir,\n",
        "                    \"class_tokens\": f\"{instance_token} {class_token}\",\n",
        "                    \"num_repeats\": train_repeats,\n",
        "                },\n",
        "                {\n",
        "                    \"is_reg\": True,\n",
        "                    \"image_dir\": reg_data_dir,\n",
        "                    \"class_tokens\": class_token,\n",
        "                    \"num_repeats\": reg_repeats,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(dataset_config, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G5u_DhFeyJ6R",
        "outputId": "6e03e5ad-2638-4138-e169-5c552c2986ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[datasets]]\n",
            "resolution = 512\n",
            "min_bucket_reso = 256\n",
            "max_bucket_reso = 1024\n",
            "caption_dropout_rate = 0\n",
            "caption_tag_dropout_rate = 0.2\n",
            "caption_dropout_every_n_epochs = 2\n",
            "flip_aug = true\n",
            "color_aug = false\n",
            "[[datasets.subsets]]\n",
            "image_dir = \"/content/LoRA/train_data\"\n",
            "class_tokens = \"jsh man\"\n",
            "num_repeats = 20\n",
            "\n",
            "[[datasets.subsets]]\n",
            "is_reg = true\n",
            "image_dir = \"/content/LoRA/reg_data\"\n",
            "class_tokens = \"man\"\n",
            "num_repeats = 1\n",
            "\n",
            "\n",
            "[general]\n",
            "enable_bucket = true\n",
            "caption_extension = \".txt\"\n",
            "shuffle_caption = true\n",
            "keep_tokens = 0\n",
            "bucket_reso_steps = 64\n",
            "bucket_no_upscale = false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.3. Sample Prompt Config\n",
        "enable_sample = True #@param {type:\"boolean\"}\n",
        "sample_every_n_type = \"sample_every_n_epochs\" #@param [\"sample_every_n_steps\", \"sample_every_n_epochs\"]\n",
        "sample_every_n_type_value = 1 #@param {type:\"number\"}\n",
        "if not enable_sample:\n",
        "  sample_every_n_type_value = 999999\n",
        "sampler = \"euler_a\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "prompt = \"best quality, ultra high res, photorealistic, handsome topless jsh man,blurry room background, \\u003Clora:jockstrap:0.0>,sitting on,yamato akira  seiji, arms behind back, \" #@param {type: \"string\"}\n",
        "negative = \"paintings, sketches, worst quality, (low quality:2), (normal quality:2), lowres, normal quality, monochrome, ((grayscale)), skin spots, acnes, skin blemishes, age spot, (bad anatomy), feminine,women,girl, anime, \" #@param {type: \"string\"\n",
        "width = \"512\" #@param {type:\"string\"}\n",
        "height = \"768\" #@param {type:\"string\"}\n",
        "scale = 7 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "steps = 28 #@param {type:\"number\"}\n",
        "\n",
        "sample_str = f\"\"\"\n",
        "  {prompt} \\\n",
        "  --n {negative} \\\n",
        "  --w {width} \\\n",
        "  --h {height} \\\n",
        "  --l {scale} \\\n",
        "  --s {steps} \\\n",
        "  {f\"--d \" + seed if seed > 0 else \"\"} \\\n",
        "\"\"\"\n",
        "\n",
        "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "\n",
        "with open(prompt_path, \"w\") as f:\n",
        "    f.write(sample_str)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QCEAEGhDG1QO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.4. LoRA and Optimizer Config\n",
        "\n",
        "#@markdown ### LoRA Config:\n",
        "#@markdown - `networks.lora` is normal and default [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts) LoRA.\n",
        "#@markdown - `lycoris.kohya` is a python package for LoRA module. Previously LoCon. Currently there are 2 LoRA algorithms: LoCon and LoRA with [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) representation. Put `algo=lora` for LoCon or `algo=loha` for Hadamard Product in `network_args`. Read: [KohakuBlueleaf/LyCORIS](https://github.com/KohakuBlueleaf/Lycoris).\n",
        "#@markdown - `locon.locon_kohya` <font color = 'red'> (backward compatibility, deprecated)</font> is LoRA for convolutional network. In short, it's the same LoRA but training almost all layers including normal LoRA layer. Read: [KohakuBlueleaf/LoCon](https://github.com/KohakuBlueleaf/LoCon).\n",
        "network_module = \"lycoris.kohya\" #@param [\"networks.lora\", \"lycoris.kohya\", \"locon.locon_kohya\"]\n",
        "\n",
        "#@markdown For custom `networks_module` you need to set additional `network_args`, e.g.: `[\"conv_dim=32\",\"conv_alpha=16\"]`\n",
        "network_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Some LoRA guides using 128 dim/alpha, but it's recommended to not specify `network_dim` and `alpha` higher than `48-64`. \n",
        "#@markdown The smaller `network_dim` is, the smaller the model size is. The larger `network_alpha` is, the closer the model is to a fully fine-tuned model. Read: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "network_dim = 32 #@param {'type':'number'}\n",
        "network_alpha = 16 #@param {'type':'number'}\n",
        "#@markdown You can specify this field for resume training.\n",
        "network_weight = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown ### <br>Optimizer Config:\n",
        "#@markdown `AdamW8bit` was the old `--use_8bit_adam`.\n",
        "optimizer_type = \"AdamW8bit\" #@param [\"AdamW\", \"AdamW8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation\", \"AdaFactor\"]\n",
        "#@markdown Additional arguments for optimizer, e.g: `[\"decouple=true\",\"weight_decay=0.6\"]`\n",
        "optimizer_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Set `unet_lr` to `1.0` if you use `DAdaptation` optimizer, because it's a [free learning rate](https://github.com/facebookresearch/dadaptation) algorithm. \n",
        "#@markdown However `text_encoder_lr = 1/2 * unet_lr` still applied, so you need to set `0.5` for `text_encoder_lr`.\n",
        "#@markdown Also actually you don't need to specify `learning_rate` value if both `unet_lr` and `text_encoder_lr` are defined.\n",
        "train_unet = True #@param {'type':'boolean'}\n",
        "unet_lr = 1e-4 #@param {'type':'number'}\n",
        "train_text_encoder = True #@param {'type':'boolean'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"] {allow-input: false}\n",
        "lr_warmup_steps = 0 #@param {'type':'number'}\n",
        "#@markdown You can define `num_cycles` value for `cosine_with_restarts` or `power` value for `polynomial` in the field below.\n",
        "lr_scheduler_num_cycles = 0 #@param {'type':'number'}\n",
        "lr_scheduler_power = 0 #@param {'type':'number'}\n",
        "\n",
        "print(\"- LoRA Config:\")\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(\"network args:\", network_args)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if not network_weight:\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weight):\n",
        "    print(\"Loading LoRA weight:\", network_weight)\n",
        "  else:\n",
        "    print(f\"{network_weight} does not exist.\")\n",
        "    network_weight = \"\"\n",
        "\n",
        "print(\"- Optimizer Config:\")\n",
        "print(f\"Using {optimizer_type} as Optimizer\")\n",
        "if optimizer_args:\n",
        "  print(f\"Optimizer Args :\", optimizer_args)\n",
        "if train_unet and train_text_encoder:\n",
        "  print(f\"Train UNet and Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "if train_unet and not train_text_encoder:\n",
        "  print(f\"Train UNet only\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "if train_text_encoder and not train_unet:\n",
        "  print(f\"Train Text Encoder only\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "print(\"Learning rate warmup steps: \", lr_warmup_steps)\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- lr_scheduler_num_cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- lr_scheduler_power: \", lr_scheduler_power)\n",
        "\n"
      ],
      "metadata": {
        "id": "iD5Ecamp4rVW",
        "cellView": "form",
        "outputId": "853beec4-4d1b-4a74-cc6d-df630d13e5f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- LoRA Config:\n",
            "Loading network module: lycoris.kohya\n",
            "network args: \n",
            "lycoris.kohya dim set to: 32\n",
            "lycoris.kohya alpha set to: 16\n",
            "No LoRA weight loaded.\n",
            "- Optimizer Config:\n",
            "Using AdamW8bit as Optimizer\n",
            "Train UNet and Text Encoder\n",
            "UNet learning rate:  0.0001\n",
            "Text encoder learning rate:  5e-05\n",
            "Learning rate warmup steps:  0\n",
            "Learning rate Scheduler: constant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.5. Training Config\n",
        "\n",
        "import toml\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "lowram = True #@param {type:\"boolean\"}\n",
        "noise_offset = 0.005 #@param {type:\"number\"}\n",
        "num_epochs = 30 #@param {type:\"number\"}\n",
        "train_batch_size = 6 #@param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_every_n_epochs\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/LoRA/logs\"\n",
        "prior_loss_weight = 1.0\n",
        "              \n",
        "os.chdir(repo_dir)\n",
        "\n",
        "config = {\n",
        "    \"model_arguments\": {\n",
        "        \"v2\": v2,\n",
        "        \"v_parameterization\": v_parameterization if v2 and v_parameterization else False,\n",
        "        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "        \"vae\": vae,\n",
        "    },\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\": False,\n",
        "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
        "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
        "        \"network_weights\": network_weight,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_args\": eval(network_args) if network_args else None,\n",
        "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
        "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
        "        \"training_comment\": None,\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\": optimizer_type,\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "        \"debug_dataset\": False,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\": output_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"save_precision\": save_precision,\n",
        "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
        "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
        "        \"save_last_n_epochs\": None,\n",
        "        \"save_state\": None,\n",
        "        \"save_last_n_epochs_state\": None,\n",
        "        \"resume\": None,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"max_token_length\": 225,\n",
        "        \"mem_eff_attn\": False,\n",
        "        \"xformers\": True,\n",
        "        \"max_train_epochs\": num_epochs,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\": gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"clip_skip\": clip_skip if not v2 else None,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
        "        \"lowram\": lowram,\n",
        "    },\n",
        "    \"sample_prompt_arguments\":{\n",
        "        \"sample_every_n_steps\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_steps\" else None,\n",
        "        \"sample_every_n_epochs\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_epochs\" else None,\n",
        "        \"sample_sampler\": sampler,\n",
        "    },\n",
        "    \"dreambooth_arguments\":{\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"saving_arguments\":{\n",
        "        \"save_model_as\": save_model_as\n",
        "    },\n",
        "}\n",
        "\n",
        "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "id": "-Z4w3lfFKLjr",
        "cellView": "form",
        "outputId": "1f61a002-6198-49ac-914c-a132792cec31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[model_arguments]\n",
            "v2 = false\n",
            "v_parameterization = false\n",
            "pretrained_model_name_or_path = \"/content/pretrained_model/chillbasil_jockstrap.safetensors\"\n",
            "\n",
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "unet_lr = 0.0001\n",
            "text_encoder_lr = 5e-5\n",
            "network_module = \"lycoris.kohya\"\n",
            "network_dim = 32\n",
            "network_alpha = 16\n",
            "network_train_unet_only = false\n",
            "network_train_text_encoder_only = false\n",
            "\n",
            "[optimizer_arguments]\n",
            "optimizer_type = \"AdamW8bit\"\n",
            "learning_rate = 0.0001\n",
            "max_grad_norm = 1.0\n",
            "lr_scheduler = \"constant\"\n",
            "lr_warmup_steps = 0\n",
            "\n",
            "[dataset_arguments]\n",
            "cache_latents = true\n",
            "debug_dataset = false\n",
            "\n",
            "[training_arguments]\n",
            "output_dir = \"/content/drive/MyDrive/LoRA/output\"\n",
            "output_name = \"004_jsh_05_w\"\n",
            "save_precision = \"fp16\"\n",
            "save_every_n_epochs = 1\n",
            "train_batch_size = 6\n",
            "max_token_length = 225\n",
            "mem_eff_attn = false\n",
            "xformers = true\n",
            "max_train_epochs = 30\n",
            "max_data_loader_n_workers = 8\n",
            "persistent_data_loader_workers = true\n",
            "gradient_checkpointing = false\n",
            "gradient_accumulation_steps = 1\n",
            "mixed_precision = \"fp16\"\n",
            "clip_skip = 2\n",
            "logging_dir = \"/content/LoRA/logs\"\n",
            "log_prefix = \"004_jsh_05_w\"\n",
            "noise_offset = 0.005\n",
            "lowram = true\n",
            "\n",
            "[sample_prompt_arguments]\n",
            "sample_every_n_epochs = 1\n",
            "sample_sampler = \"euler_a\"\n",
            "\n",
            "[dreambooth_arguments]\n",
            "prior_loss_weight = 1.0\n",
            "\n",
            "[saving_arguments]\n",
            "save_model_as = \"safetensors\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.6. Start Training\n",
        "\n",
        "#@markdown Check your config here if you want to edit something: \n",
        "#@markdown - `sample_prompt` : /content/LoRA/config/sample_prompt.txt\n",
        "#@markdown - `config_file` : /content/LoRA/config/config_file.toml\n",
        "#@markdown - `dataset_config` : /content/LoRA/config/dataset_config.toml\n",
        "\n",
        "#@markdown Generated sample can be seen here: /content/LoRA/output/sample\n",
        "\n",
        "#@markdown You can import config from another session if you want.\n",
        "sample_prompt = \"/content/LoRA/config/sample_prompt.txt\" #@param {type:'string'}\n",
        "config_file = \"/content/LoRA/config/config_file.toml\" #@param {type:'string'}\n",
        "dataset_config = \"/content/LoRA/config/dataset_config.toml\" #@param {type:'string'}\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!accelerate launch \\\n",
        "  --config_file={accelerate_config} \\\n",
        "  --num_cpu_threads_per_process=1 \\\n",
        "  train_network.py \\\n",
        "  --sample_prompts={sample_prompt} \\\n",
        "  --dataset_config={dataset_config} \\\n",
        "  --config_file={config_file}\n",
        "\n"
      ],
      "metadata": {
        "id": "p_SHtbFwHVl1",
        "cellView": "form",
        "outputId": "e44f91e6-42f3-405e-cc03-4d298f6e1aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-17 11:49:10.446752: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-03-17 11:49:11.196975: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 11:49:11.197070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 11:49:11.197088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-17 11:49:14.111627: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-03-17 11:49:14.850762: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 11:49:14.850867: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-17 11:49:14.850887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Loading settings from /content/LoRA/config/config_file.toml...\n",
            "/content/LoRA/config/config_file\n",
            "prepare tokenizer\n",
            "update token length: 225\n",
            "Load dataset config from /content/LoRA/config/dataset_config.toml\n",
            "prepare images.\n",
            "found directory /content/LoRA/train_data contains 5 image files\n",
            "found directory /content/LoRA/reg_data contains 103 image files\n",
            "100 train images with repeating.\n",
            "103 reg images.\n",
            "some of reg images are not used / 正則化画像の数が多いので、一部使用されない正則化画像があります\n",
            "[Dataset 0]\n",
            "  batch_size: 6\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/LoRA/train_data\"\n",
            "    image_count: 5\n",
            "    num_repeats: 20\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0\n",
            "    caption_dropout_every_n_epoches: 2\n",
            "    caption_tag_dropout_rate: 0.2\n",
            "    color_aug: False\n",
            "    flip_aug: True\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    is_reg: False\n",
            "    class_tokens: jsh man\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 1 of Dataset 0]\n",
            "    image_dir: \"/content/LoRA/reg_data\"\n",
            "    image_count: 103\n",
            "    num_repeats: 1\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0\n",
            "    caption_dropout_every_n_epoches: 2\n",
            "    caption_tag_dropout_rate: 0.2\n",
            "    color_aug: False\n",
            "    flip_aug: True\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    is_reg: True\n",
            "    class_tokens: man\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 105/105 [00:00<00:00, 10204.88it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (448, 576), count: 200\n",
            "mean ar error (without repeats): 0.07465277777777779\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Downloading (…)lve/main/config.json: 100% 4.52k/4.52k [00:00<00:00, 711kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 1.71G/1.71G [00:12<00:00, 139MB/s]\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "loading text encoder: <All keys matched successfully>\n",
            "Replace CrossAttention.forward to use xformers\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "100% 105/105 [00:15<00:00,  6.57it/s]\n",
            "import network module: lycoris.kohya\n",
            "Using rank adaptation algo: lora\n",
            "Use Dropout value: 0.0\n",
            "Create LoCon Module\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "Create LoCon Module\n",
            "create LoRA for U-Net: 278 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "use 8-bit AdamW optimizer | {}\n",
            "override steps. steps for 30 epochs is / 指定エポックまでのステップ数: 1020\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 100\n",
            "  num reg images / 正則化画像の数: 103\n",
            "  num batches per epoch / 1epochのバッチ数: 34\n",
            "  num epochs / epoch数: 30\n",
            "  batch size per device / バッチサイズ: 6\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 1020\n",
            "steps:   0% 0/1020 [00:00<?, ?it/s]epoch 1/30\n",
            "steps:   3% 34/1020 [00:15<07:43,  2.13it/s, loss=0.051]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000001.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 34\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.94it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  6.59it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:03,  7.41it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:03,  7.83it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.11it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.28it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.31it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:02,  8.33it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.34it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.14it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  7.89it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:02,  7.70it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  7.53it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  7.35it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  7.33it/s]\u001b[A\n",
            " 57% 16/28 [00:02<00:01,  7.43it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  7.69it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  7.89it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.06it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.19it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.16it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.27it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.28it/s]\u001b[A\n",
            " 86% 24/28 [00:03<00:00,  8.37it/s]\u001b[A\n",
            " 89% 25/28 [00:03<00:00,  8.45it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.48it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.52it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  7.98it/s]\n",
            "epoch 2/30\n",
            "steps:   7% 68/1020 [00:35<08:11,  1.94it/s, loss=0.0615]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000002.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 68\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.57it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.56it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.49it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.54it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.56it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.57it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.58it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.58it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.50it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.49it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.47it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.47it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.30it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.32it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.36it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.34it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.40it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.34it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.32it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.39it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.44it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.45it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.45it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.45it/s]\n",
            "epoch 3/30\n",
            "steps:  10% 102/1020 [00:53<08:02,  1.90it/s, loss=0.0514]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000003.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 102\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.24it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.29it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.34it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.32it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.24it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.29it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.48it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.50it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:01,  8.50it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.51it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.52it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.52it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.54it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.54it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.39it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.40it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.43it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.43it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.43it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.45it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.44it/s]\n",
            "epoch 4/30\n",
            "steps:  13% 136/1020 [01:12<07:48,  1.89it/s, loss=0.0474]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000004.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 136\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.05it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.32it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.46it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.44it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.35it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.41it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.39it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.42it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.46it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.32it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.24it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.17it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.25it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.30it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.36it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.40it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.43it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.48it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.48it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.50it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.40it/s]\n",
            "epoch 5/30\n",
            "steps:  17% 170/1020 [01:30<07:33,  1.87it/s, loss=0.0384]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000005.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 170\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.38it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.45it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.49it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.47it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.46it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.45it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.44it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.46it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.21it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.31it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.36it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.33it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.37it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.37it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.41it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.44it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.38it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.43it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.45it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.49it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.42it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.40it/s]\n",
            "epoch 6/30\n",
            "steps:  20% 204/1020 [01:49<07:17,  1.87it/s, loss=0.049] saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000006.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 204\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.36it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.49it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.54it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.55it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.46it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.47it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.46it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.47it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:01,  8.50it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.49it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.47it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.49it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.51it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.53it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.53it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.54it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.55it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.55it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.57it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.53it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.53it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.53it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.51it/s]\n",
            "epoch 7/30\n",
            "steps:  23% 238/1020 [02:07<07:00,  1.86it/s, loss=0.0519]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000007.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 238\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.13it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.18it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:03,  8.31it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.37it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.42it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.43it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.45it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.39it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.39it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.39it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.38it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.21it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.22it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.27it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.32it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.26it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.26it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.16it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.27it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.34it/s]\n",
            "epoch 8/30\n",
            "steps:  27% 272/1020 [02:26<06:43,  1.85it/s, loss=0.0509]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000008.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 272\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.41it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.35it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.39it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.50it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.49it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.53it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.50it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.49it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.49it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.52it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.51it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.47it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.44it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.47it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.39it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.38it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.40it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.44it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.47it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.49it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.47it/s]\n",
            "epoch 9/30\n",
            "steps:  30% 306/1020 [02:45<06:26,  1.85it/s, loss=0.056] saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000009.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 306\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.31it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.40it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.37it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.40it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.42it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.45it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.47it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.52it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.44it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.46it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.40it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.40it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.34it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.36it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.33it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.33it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.34it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.40it/s]\n",
            "epoch 10/30\n",
            "steps:  33% 340/1020 [03:04<06:08,  1.85it/s, loss=0.0488]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000010.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 340\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.56it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.33it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.26it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.33it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.38it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.38it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.36it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.35it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.32it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.30it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.28it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.29it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.29it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.28it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.29it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.33it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.29it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.32it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.32it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.34it/s]\u001b[A\n",
            " 89% 25/28 [00:03<00:00,  8.37it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.38it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.40it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.34it/s]\n",
            "epoch 11/30\n",
            "steps:  37% 374/1020 [03:22<05:50,  1.84it/s, loss=0.0519]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000011.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 374\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.34it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.37it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.44it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.39it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.37it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.33it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.38it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.42it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.34it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.37it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.39it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.15it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.24it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.26it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.30it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.38it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.40it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.41it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.44it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.41it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.37it/s]\n",
            "epoch 12/30\n",
            "steps:  40% 408/1020 [03:41<05:32,  1.84it/s, loss=0.0542]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000012.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 408\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.34it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.35it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:03,  8.08it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.14it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.15it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.17it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.19it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.24it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.25it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.30it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.28it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.30it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.34it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.37it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.50it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.52it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.56it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.59it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.61it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.59it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.59it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.62it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.63it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.65it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.67it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.68it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.45it/s]\n",
            "epoch 13/30\n",
            "steps:  43% 442/1020 [04:00<05:14,  1.84it/s, loss=0.052] saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000013.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 442\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.56it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.57it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.59it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.61it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.59it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.59it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.59it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.60it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.61it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.61it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:01,  8.61it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.62it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.58it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.55it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.50it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.37it/s]\u001b[A\n",
            " 61% 17/28 [00:01<00:01,  8.38it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.39it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.38it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.38it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.43it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.48it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.54it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.58it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.59it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.52it/s]\n",
            "epoch 14/30\n",
            "steps:  47% 476/1020 [04:19<04:56,  1.84it/s, loss=0.0483]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000014.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 476\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.38it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.35it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.26it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.31it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.41it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.43it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.43it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.46it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.45it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.42it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.41it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.35it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.38it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.42it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.47it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.50it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.56it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.58it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.61it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.45it/s]\n",
            "epoch 15/30\n",
            "steps:  50% 510/1020 [04:37<04:37,  1.84it/s, loss=0.042] saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000015.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 510\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.39it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.35it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.39it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.50it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.50it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.48it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.41it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.43it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.47it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.51it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.54it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.54it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.54it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.54it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.40it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.43it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.14it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.24it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.33it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.41it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.46it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.48it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.43it/s]\n",
            "epoch 16/30\n",
            "steps:  53% 544/1020 [04:56<04:19,  1.83it/s, loss=0.0437]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000016.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 544\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.38it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.46it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.49it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.51it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.52it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.51it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.53it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.38it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.40it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.46it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.46it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.49it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.50it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.51it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.46it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.48it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.47it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.48it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.47it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.45it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.47it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.46it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.46it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.46it/s]\n",
            "epoch 17/30\n",
            "steps:  57% 578/1020 [05:15<04:01,  1.83it/s, loss=0.048] saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000017.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 578\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.45it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.49it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.51it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.52it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.44it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.49it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.51it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.53it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.48it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.49it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.35it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.40it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.48it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.51it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.57it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.57it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.58it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.56it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.54it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.53it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.47it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.48it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.46it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.44it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.49it/s]\n",
            "epoch 18/30\n",
            "steps:  60% 612/1020 [05:33<03:42,  1.83it/s, loss=0.0405]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000018.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 612\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.31it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.43it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.33it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.34it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.40it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.43it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:01,  8.51it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.51it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.58it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.46it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.49it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.47it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.26it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.32it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.38it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.38it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.43it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.47it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.32it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.41it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.32it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.40it/s]\n",
            "epoch 19/30\n",
            "steps:  63% 646/1020 [05:52<03:24,  1.83it/s, loss=0.0451]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000019.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 646\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.31it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.36it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.39it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.30it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.32it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.31it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.29it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.31it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.29it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.27it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.11it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.13it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.15it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.20it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.20it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.23it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.23it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.24it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.26it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.26it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.26it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.26it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.24it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.25it/s]\u001b[A\n",
            " 89% 25/28 [00:03<00:00,  8.25it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.25it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.09it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.22it/s]\n",
            "epoch 20/30\n",
            "steps:  67% 680/1020 [06:11<03:05,  1.83it/s, loss=0.0499]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000020.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 680\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.37it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.35it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.33it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.32it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.30it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.29it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.36it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.35it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.36it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.39it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.46it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.48it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.39it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.44it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.45it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.47it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.48it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.33it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.35it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.36it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.39it/s]\n",
            "epoch 21/30\n",
            "steps:  70% 714/1020 [06:29<02:47,  1.83it/s, loss=0.0564]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000021.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 714\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.33it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.06it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:03,  8.23it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.33it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.39it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.40it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.32it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.34it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.15it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  7.90it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  7.95it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  7.98it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.15it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.25it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.35it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.41it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.45it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.49it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.51it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.54it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.54it/s]\u001b[A\n",
            " 89% 25/28 [00:03<00:00,  8.53it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.52it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.36it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.33it/s]\n",
            "epoch 22/30\n",
            "steps:  73% 748/1020 [06:48<02:28,  1.83it/s, loss=0.0595]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000022.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 748\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.29it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.37it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.44it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.44it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.41it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.40it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.45it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.47it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.49it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.47it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.48it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.46it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.49it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.49it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.50it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.48it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.25it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.30it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.36it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.23it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.28it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.28it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.29it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.39it/s]\n",
            "epoch 23/30\n",
            "steps:  77% 782/1020 [07:07<02:09,  1.83it/s, loss=0.0514]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000023.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 782\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.43it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.39it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.30it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.40it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.39it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.42it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.44it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.39it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.41it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.45it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.46it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.48it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.49it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.49it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.52it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.52it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.50it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.51it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.37it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.43it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.43it/s]\n",
            "epoch 24/30\n",
            "steps:  80% 816/1020 [07:25<01:51,  1.83it/s, loss=0.0494]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000024.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 816\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.54it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.60it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.54it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.60it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.64it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.67it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.67it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.67it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.45it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.46it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.46it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.55it/s]\u001b[A\n",
            " 61% 17/28 [00:01<00:01,  8.54it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.44it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.46it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.49it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.41it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.53it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.57it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.58it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.61it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.60it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.53it/s]\n",
            "epoch 25/30\n",
            "steps:  83% 850/1020 [07:44<01:32,  1.83it/s, loss=0.0387]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000025.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 850\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.29it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.39it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.38it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.16it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.26it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.30it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.30it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.28it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.34it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.38it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.40it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.42it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.46it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.48it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.38it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.36it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.41it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.45it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.48it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.42it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.38it/s]\n",
            "epoch 26/30\n",
            "steps:  87% 884/1020 [08:03<01:14,  1.83it/s, loss=0.0496]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000026.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 884\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.38it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.39it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.31it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.42it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.41it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.45it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.40it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.39it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.41it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.37it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.47it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.50it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.56it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.59it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.60it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.62it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.58it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.43it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.42it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.43it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.45it/s]\n",
            "epoch 27/30\n",
            "steps:  90% 918/1020 [08:21<00:55,  1.83it/s, loss=0.0468]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000027.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 918\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.39it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.21it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:03,  7.97it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.13it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.28it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.20it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.29it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.36it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.31it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.35it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.33it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.38it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.35it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.38it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.41it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.43it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.39it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.42it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.41it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.39it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.40it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.46it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.49it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.49it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.37it/s]\n",
            "epoch 28/30\n",
            "steps:  93% 952/1020 [08:40<00:37,  1.83it/s, loss=0.0558]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000028.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 952\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.26it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.34it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.44it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.42it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.51it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.51it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.53it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.54it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.56it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.53it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.52it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.15it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.06it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.19it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.28it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.35it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  7.99it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  7.81it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  7.85it/s]\u001b[A\n",
            " 89% 25/28 [00:03<00:00,  7.99it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  7.80it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  7.67it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.17it/s]\n",
            "epoch 29/30\n",
            "steps:  97% 986/1020 [08:59<00:18,  1.83it/s, loss=0.0444]saving checkpoint: /content/drive/MyDrive/LoRA/output/004_jsh_05_w-000029.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 986\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.39it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.42it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.44it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.46it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.35it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.37it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.37it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.41it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.37it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.40it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.43it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.42it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.37it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.41it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.43it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.43it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.44it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.45it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.41it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.42it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.36it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.39it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.39it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.28it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.39it/s]\n",
            "epoch 30/30\n",
            "steps: 100% 1020/1020 [09:18<00:00,  1.83it/s, loss=0.0443]generating sample images at step / サンプル画像生成 ステップ: 1020\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:03,  8.30it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:03,  8.35it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:02,  8.34it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:02,  8.28it/s]\u001b[A\n",
            " 18% 5/28 [00:00<00:02,  8.26it/s]\u001b[A\n",
            " 21% 6/28 [00:00<00:02,  8.34it/s]\u001b[A\n",
            " 25% 7/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
            " 29% 8/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:02,  8.42it/s]\u001b[A\n",
            " 36% 10/28 [00:01<00:02,  8.44it/s]\u001b[A\n",
            " 39% 11/28 [00:01<00:02,  8.15it/s]\u001b[A\n",
            " 43% 12/28 [00:01<00:01,  8.22it/s]\u001b[A\n",
            " 46% 13/28 [00:01<00:01,  8.28it/s]\u001b[A\n",
            " 50% 14/28 [00:01<00:01,  8.27it/s]\u001b[A\n",
            " 54% 15/28 [00:01<00:01,  8.32it/s]\u001b[A\n",
            " 57% 16/28 [00:01<00:01,  8.35it/s]\u001b[A\n",
            " 61% 17/28 [00:02<00:01,  8.30it/s]\u001b[A\n",
            " 64% 18/28 [00:02<00:01,  8.36it/s]\u001b[A\n",
            " 68% 19/28 [00:02<00:01,  8.34it/s]\u001b[A\n",
            " 71% 20/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 75% 21/28 [00:02<00:00,  8.34it/s]\u001b[A\n",
            " 79% 22/28 [00:02<00:00,  8.34it/s]\u001b[A\n",
            " 82% 23/28 [00:02<00:00,  8.36it/s]\u001b[A\n",
            " 86% 24/28 [00:02<00:00,  8.37it/s]\u001b[A\n",
            " 89% 25/28 [00:02<00:00,  8.39it/s]\u001b[A\n",
            " 93% 26/28 [00:03<00:00,  8.45it/s]\u001b[A\n",
            " 96% 27/28 [00:03<00:00,  8.50it/s]\u001b[A\n",
            "100% 28/28 [00:03<00:00,  8.36it/s]\n",
            "save trained model to /content/drive/MyDrive/LoRA/output/004_jsh_05_w.safetensors\n",
            "model saved.\n",
            "steps: 100% 1020/1020 [09:22<00:00,  1.81it/s, loss=0.0443]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. Testing"
      ],
      "metadata": {
        "id": "reMcN0bM_o53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.4. Visualize loss graph (Optional)\n",
        "training_logs_path = \"/content/LoRA/logs\" #@param {type : \"string\"}\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {training_logs_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TLSQslfFcQde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}