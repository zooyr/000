{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zooyr/000/blob/master/004_jsh_03__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/drive/MyDrive/models/* /content/pretrained_model"
      ],
      "metadata": {
        "id": "Oc4r-GYI2B7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# I. Install Kohya Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3q60di584x",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ## 1.1. Install Dependencies\n",
        "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.  This will also install the required libraries.\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "#root_dir\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir,\"deps\")\n",
        "repo_dir = os.path.join(root_dir,\"kohya-trainer\")\n",
        "training_dir = os.path.join(root_dir,\"LoRA\")\n",
        "pretrained_model = os.path.join(root_dir,\"pretrained_model\")\n",
        "vae_dir = os.path.join(root_dir,\"vae\")\n",
        "config_dir = os.path.join(training_dir,\"config\")\n",
        "\n",
        "#repo_dir\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir,\"tools\")\n",
        "finetune_dir = os.path.join(repo_dir,\"finetune\")\n",
        "\n",
        "for store in [\"root_dir\", \"deps_dir\", \"repo_dir\", \"training_dir\", \"pretrained_model\", \"vae_dir\", \"accelerate_config\", \"tools_dir\", \"finetune_dir\", \"config_dir\"]:\n",
        "  with capture.capture_output() as cap:\n",
        "    %store {store}\n",
        "    del cap\n",
        "\n",
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "branch = \"\" #@param {type: \"string\"}\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "mount_drive = True #@param {type: \"boolean\"}\n",
        "\n",
        "if mount_drive:\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "for dir in [deps_dir, training_dir, config_dir, pretrained_model, vae_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "  \n",
        "def clone_repo(url):\n",
        "  if not os.path.exists(repo_dir):\n",
        "    os.chdir(root_dir)\n",
        "    !git clone {url} {repo_dir}\n",
        "  else:\n",
        "    os.chdir(repo_dir)\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "\n",
        "clone_repo(repo_url)\n",
        "\n",
        "if branch:\n",
        "  os.chdir(repo_dir)\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "  with capture.capture_output() as cap:\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(name, 'r') as deps:\n",
        "      deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "    del cap \n",
        "\n",
        "def install_dependencies():\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "  if install_xformers:\n",
        "    !pip install -q --pre xformers\n",
        "    !pip install -q --pre triton\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "install_dependencies()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gob9_OwTlwh"
      },
      "source": [
        "# II. Pretrained Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En9UUwGNMRMM"
      },
      "source": [
        "# III. Data Acquisition\n",
        "\n",
        "You have two options for acquiring your dataset: uploading it to this notebook or bulk downloading images from Danbooru using the image scraper. If you prefer to use your own dataset, simply upload it to Colab's local files.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.1. Locating Train Data Directory\n",
        "#@markdown Define the location of your training data. This cell will also create a folder based on your input. Regularization Images is `optional` and can be skipped.\n",
        "import os\n",
        "from IPython.utils import capture\n",
        "%store -r\n",
        "\n",
        "train_data_dir = \"/content/LoRA/train_data\" #@param {type:'string'}\n",
        "reg_data_dir = \"/content/LoRA/reg_data\" #@param {type:'string'}\n",
        "\n",
        "for image_dir in [train_data_dir, reg_data_dir]:\n",
        "  if image_dir:\n",
        "    with capture.capture_output() as cap:\n",
        "      os.makedirs(image_dir, exist_ok=True)\n",
        "      %store image_dir\n",
        "      del cap\n",
        "\n",
        "print(f\"Your train data directory : {train_data_dir}\")\n",
        "if reg_data_dir:\n",
        "  print(f\"Your reg data directory : {reg_data_dir}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y",
        "outputId": "abd35eff-6429-4923-9e77-5e2734c2f245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your train data directory : /content/LoRA/train_data\n",
            "Your reg data directory : /content/LoRA/reg_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Unzip Dataset\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "\n",
        "#@markdown Specify this section if your dataset is in a `zip` file and has been uploaded somewhere. This will download your dataset and automatically extract it to the `train_data_dir` if the `unzip_to` is empty. \n",
        "zipfile_url = \"/content/drive/MyDrive/004_jsh_03.zip\" #@param {'type': 'string'}\n",
        "zipfile_name = \"zipfile.zip\"\n",
        "unzip_to = \"\" #@param {'type': 'string'}\n",
        "\n",
        "hf_token = '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'\n",
        "user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "\n",
        "if unzip_to:\n",
        "  os.makedirs(unzip_to, exist_ok=True)\n",
        "else:\n",
        "  unzip_to = train_data_dir\n",
        "\n",
        "def download_dataset(url):\n",
        "  if url.startswith(\"/content\"):\n",
        "    !unzip -j -o {url} -d \"{train_data_dir}\"\n",
        "  elif url.startswith(\"https://drive.google.com\"):\n",
        "    os.chdir(root_dir)\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "\n",
        "download_dataset(zipfile_url)\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "if not zipfile_url.startswith(\"/content\"):\n",
        "  !unzip -j -o \"{root_dir}/{zipfile_name}\" -d \"{unzip_to}\"\n",
        "  os.remove(f\"{root_dir}/{zipfile_name}\")\n",
        "\n",
        "files_to_move = (\"meta_cap.json\", \\\n",
        "                 \"meta_cap_dd.json\", \\\n",
        "                 \"meta_lat.json\", \\\n",
        "                 \"meta_clean.json\")\n",
        "\n",
        "for filename in os.listdir(train_data_dir):\n",
        "  file_path = os.path.join(train_data_dir, filename)\n",
        "  if filename in files_to_move:\n",
        "    if not os.path.exists(file_path):\n",
        "      shutil.move(file_path, training_dir)\n",
        "    else: \n",
        "      os.remove(file_path)"
      ],
      "metadata": {
        "id": "eFFHVTWNZGbp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Unzip Dataset\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "\n",
        "#@markdown Specify this section if your dataset is in a `zip` file and has been uploaded somewhere. This will download your dataset and automatically extract it to the `train_data_dir` if the `unzip_to` is empty. \n",
        "zipfile_url = \"/content/drive/MyDrive/004_jsh_03.zip\" #@param {'type': 'string'}\n",
        "zipfile_name = \"zipfile.zip\"\n",
        "unzip_to = \"\" #@param {'type': 'string'}\n",
        "\n",
        "hf_token = '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'\n",
        "user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "\n",
        "if unzip_to:\n",
        "  os.makedirs(unzip_to, exist_ok=True)\n",
        "else:\n",
        "  unzip_to = reg_data_dir\n",
        "\n",
        "def download_dataset(url):\n",
        "  if url.startswith(\"/content\"):\n",
        "    !unzip -j -o {url} -d \"{train_data_dir}\"\n",
        "  elif url.startswith(\"https://drive.google.com\"):\n",
        "    os.chdir(root_dir)\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "\n",
        "download_dataset(zipfile_url)\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "if not zipfile_url.startswith(\"/content\"):\n",
        "  !unzip -j -o \"{root_dir}/{zipfile_name}\" -d \"{unzip_to}\"\n",
        "  os.remove(f\"{root_dir}/{zipfile_name}\")\n",
        "\n",
        "files_to_move = (\"meta_cap.json\", \\\n",
        "                 \"meta_cap_dd.json\", \\\n",
        "                 \"meta_lat.json\", \\\n",
        "                 \"meta_clean.json\")\n",
        "\n",
        "for filename in os.listdir(train_data_dir):\n",
        "  file_path = os.path.join(train_data_dir, filename)\n",
        "  if filename in files_to_move:\n",
        "    if not os.path.exists(file_path):\n",
        "      shutil.move(file_path, training_dir)\n",
        "    else: \n",
        "      os.remove(file_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WMrjqNzAQcvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "p = pathlib.Path('/content/LoRA/train_data').glob('000*') "
      ],
      "metadata": {
        "id": "lXdvRsn70rgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V30hG_j4x_cP",
        "outputId": "c9c0fed7-2287-4e49-ccd5-8a6b6672919e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "for i in p:\n",
        "  shutil.move(i, '/content/LoRA/reg_data')\n"
      ],
      "metadata": {
        "id": "eVKz_ivQ1qIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = pathlib.Path('/content/LoRA/train_data').glob('*.txt') \n",
        "sorted(p)"
      ],
      "metadata": {
        "id": "hdub34IjcK5p",
        "outputId": "c10c5f16-7a42-418e-9c35-739ecc2b2acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/content/LoRA/train_data/f01.txt'),\n",
              " PosixPath('/content/LoRA/train_data/f02.txt'),\n",
              " PosixPath('/content/LoRA/train_data/f03.txt'),\n",
              " PosixPath('/content/LoRA/train_data/f04.txt'),\n",
              " PosixPath('/content/LoRA/train_data/f05.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in p:\n",
        "  print(i.name)"
      ],
      "metadata": {
        "id": "EgLtWhdRcYAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Data Preprocessing"
      ],
      "metadata": {
        "id": "T-0qKyEgTchp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Training Model\n",
        "\n"
      ],
      "metadata": {
        "id": "yHNbl3O_NSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.1. Model Config\n",
        "from google.colab import drive\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "project_name = \"004_jsh_04\" #@param {type:\"string\"}\n",
        "if not project_name:\n",
        "  project_name = \"last\"\n",
        "pretrained_model_name_or_path = \"/content/pretrained_model/chillbasil_jockstrap.safetensors\" #@param {type:\"string\"}\n",
        "vae = \"\"  #@param {type:\"string\"}\n",
        "output_dir = \"/content/LoRA/output\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/LoRA/output` by default\n",
        "output_to_drive = True #@param {'type':'boolean'}\n",
        "\n",
        "if output_to_drive:\n",
        "  output_dir = \"/content/drive/MyDrive/LoRA/output\"\n",
        "\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')  \n",
        "\n",
        "sample_dir = os.path.join(output_dir, \"sample\")\n",
        "for dir in [output_dir, sample_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(\"Project Name: \", project_name)\n",
        "print(\"Model Version: Stable Diffusion V1.x\") if not v2 else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x\") if v2 and not v_parameterization else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x 768v\") if v2 and v_parameterization else \"\"\n",
        "print(\"Pretrained Model Path: \", pretrained_model_name_or_path) if pretrained_model_name_or_path else print(\"No Pretrained Model path specified.\")\n",
        "print(\"VAE Path: \", vae) if vae else print(\"No VAE path specified.\")\n",
        "print(\"Output Path: \", output_dir)"
      ],
      "metadata": {
        "id": "H_Q23fUEJhnC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.2. Dataset Config\n",
        "import toml\n",
        "\n",
        "#@markdown ### Dreambooth Config\n",
        "train_repeats = 20 #@param {type:\"number\"}\n",
        "reg_repeats = 1 #@param {type:\"number\"}\n",
        "instance_token = \"jsh\" #@param {type:\"string\"}\t\n",
        "class_token = \"man\" #@param {type:\"string\"}\t \n",
        "#@markdown ### <br>General Config\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "flip_aug = True #@param {type:\"boolean\"}\t\n",
        "caption_extension = \".txt\" #@param [\"none\", \".txt\", \".caption\"]\t\n",
        "caption_dropout_rate = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\t\n",
        "caption_dropout_every_n_epochs = 2 #@param {type:\"number\"}\n",
        "keep_tokens = 0 #@param {type:\"number\"}\n",
        "\n",
        "config = {\n",
        "    \"general\": {\n",
        "        \"enable_bucket\": True,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"resolution\": resolution,\n",
        "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,           \n",
        "            \"caption_dropout_rate\": caption_dropout_rate if caption_extension == \".caption\" else 0,\n",
        "            \"caption_tag_dropout_rate\": caption_dropout_rate if caption_extension == \".txt\" else 0,\n",
        "            \"caption_dropout_every_n_epochs\": caption_dropout_every_n_epochs,\n",
        "            \"flip_aug\": flip_aug,\n",
        "            \"color_aug\": False,\n",
        "            \"face_crop_aug_range\": None,\n",
        "            \"subsets\": [\n",
        "                {\n",
        "                    \"image_dir\": train_data_dir,\n",
        "                    \"class_tokens\": f\"{instance_token} {class_token}\",\n",
        "                    \"num_repeats\": train_repeats,\n",
        "                },\n",
        "                {\n",
        "                    \"is_reg\": True,\n",
        "                    \"image_dir\": reg_data_dir,\n",
        "                    \"class_tokens\": class_token,\n",
        "                    \"num_repeats\": reg_repeats,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(dataset_config, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G5u_DhFeyJ6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.3. Sample Prompt Config\n",
        "enable_sample = True #@param {type:\"boolean\"}\n",
        "sample_every_n_type = \"sample_every_n_epochs\" #@param [\"sample_every_n_steps\", \"sample_every_n_epochs\"]\n",
        "sample_every_n_type_value = 1 #@param {type:\"number\"}\n",
        "if not enable_sample:\n",
        "  sample_every_n_type_value = 999999\n",
        "sampler = \"euler_a\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "prompt = \"best quality, ultra high res, photorealistic, handsome topless jsh man,blurry room background, \\u003Clora:jockstrap:0.0>,sitting on,yamato akira  seiji, arms behind back, \" #@param {type: \"string\"}\n",
        "negative = \"paintings, sketches, worst quality, (low quality:2), (normal quality:2), lowres, normal quality, monochrome, ((grayscale)), skin spots, acnes, skin blemishes, age spot, (bad anatomy), feminine,women,girl, anime, \" #@param {type: \"string\"\n",
        "width = \"512\" #@param {type:\"string\"}\n",
        "height = \"768\" #@param {type:\"string\"}\n",
        "scale = 7 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "steps = 28 #@param {type:\"number\"}\n",
        "\n",
        "sample_str = f\"\"\"\n",
        "  {prompt} \\\n",
        "  --n {negative} \\\n",
        "  --w {width} \\\n",
        "  --h {height} \\\n",
        "  --l {scale} \\\n",
        "  --s {steps} \\\n",
        "  {f\"--d \" + seed if seed > 0 else \"\"} \\\n",
        "\"\"\"\n",
        "\n",
        "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "\n",
        "with open(prompt_path, \"w\") as f:\n",
        "    f.write(sample_str)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QCEAEGhDG1QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.4. LoRA and Optimizer Config\n",
        "\n",
        "#@markdown ### LoRA Config:\n",
        "#@markdown - `networks.lora` is normal and default [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts) LoRA.\n",
        "#@markdown - `lycoris.kohya` is a python package for LoRA module. Previously LoCon. Currently there are 2 LoRA algorithms: LoCon and LoRA with [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) representation. Put `algo=lora` for LoCon or `algo=loha` for Hadamard Product in `network_args`. Read: [KohakuBlueleaf/LyCORIS](https://github.com/KohakuBlueleaf/Lycoris).\n",
        "#@markdown - `locon.locon_kohya` <font color = 'red'> (backward compatibility, deprecated)</font> is LoRA for convolutional network. In short, it's the same LoRA but training almost all layers including normal LoRA layer. Read: [KohakuBlueleaf/LoCon](https://github.com/KohakuBlueleaf/LoCon).\n",
        "network_module = \"lycoris.kohya\" #@param [\"networks.lora\", \"lycoris.kohya\", \"locon.locon_kohya\"]\n",
        "\n",
        "#@markdown For custom `networks_module` you need to set additional `network_args`, e.g.: `[\"conv_dim=32\",\"conv_alpha=16\"]`\n",
        "network_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Some LoRA guides using 128 dim/alpha, but it's recommended to not specify `network_dim` and `alpha` higher than `48-64`. \n",
        "#@markdown The smaller `network_dim` is, the smaller the model size is. The larger `network_alpha` is, the closer the model is to a fully fine-tuned model. Read: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "network_dim = 32 #@param {'type':'number'}\n",
        "network_alpha = 16 #@param {'type':'number'}\n",
        "#@markdown You can specify this field for resume training.\n",
        "network_weight = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown ### <br>Optimizer Config:\n",
        "#@markdown `AdamW8bit` was the old `--use_8bit_adam`.\n",
        "optimizer_type = \"AdamW8bit\" #@param [\"AdamW\", \"AdamW8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation\", \"AdaFactor\"]\n",
        "#@markdown Additional arguments for optimizer, e.g: `[\"decouple=true\",\"weight_decay=0.6\"]`\n",
        "optimizer_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Set `unet_lr` to `1.0` if you use `DAdaptation` optimizer, because it's a [free learning rate](https://github.com/facebookresearch/dadaptation) algorithm. \n",
        "#@markdown However `text_encoder_lr = 1/2 * unet_lr` still applied, so you need to set `0.5` for `text_encoder_lr`.\n",
        "#@markdown Also actually you don't need to specify `learning_rate` value if both `unet_lr` and `text_encoder_lr` are defined.\n",
        "train_unet = True #@param {'type':'boolean'}\n",
        "unet_lr = 1e-4 #@param {'type':'number'}\n",
        "train_text_encoder = True #@param {'type':'boolean'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"] {allow-input: false}\n",
        "lr_warmup_steps = 0 #@param {'type':'number'}\n",
        "#@markdown You can define `num_cycles` value for `cosine_with_restarts` or `power` value for `polynomial` in the field below.\n",
        "lr_scheduler_num_cycles = 0 #@param {'type':'number'}\n",
        "lr_scheduler_power = 0 #@param {'type':'number'}\n",
        "\n",
        "print(\"- LoRA Config:\")\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(\"network args:\", network_args)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if not network_weight:\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weight):\n",
        "    print(\"Loading LoRA weight:\", network_weight)\n",
        "  else:\n",
        "    print(f\"{network_weight} does not exist.\")\n",
        "    network_weight = \"\"\n",
        "\n",
        "print(\"- Optimizer Config:\")\n",
        "print(f\"Using {optimizer_type} as Optimizer\")\n",
        "if optimizer_args:\n",
        "  print(f\"Optimizer Args :\", optimizer_args)\n",
        "if train_unet and train_text_encoder:\n",
        "  print(f\"Train UNet and Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "if train_unet and not train_text_encoder:\n",
        "  print(f\"Train UNet only\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "if train_text_encoder and not train_unet:\n",
        "  print(f\"Train Text Encoder only\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "print(\"Learning rate warmup steps: \", lr_warmup_steps)\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- lr_scheduler_num_cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- lr_scheduler_power: \", lr_scheduler_power)\n",
        "\n"
      ],
      "metadata": {
        "id": "iD5Ecamp4rVW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.5. Training Config\n",
        "\n",
        "import toml\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "lowram = True #@param {type:\"boolean\"}\n",
        "noise_offset = 0.05 #@param {type:\"number\"}\n",
        "num_epochs = 30 #@param {type:\"number\"}\n",
        "train_batch_size = 3 #@param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_every_n_epochs\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/LoRA/logs\"\n",
        "prior_loss_weight = 1.0\n",
        "              \n",
        "os.chdir(repo_dir)\n",
        "\n",
        "config = {\n",
        "    \"model_arguments\": {\n",
        "        \"v2\": v2,\n",
        "        \"v_parameterization\": v_parameterization if v2 and v_parameterization else False,\n",
        "        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "        \"vae\": vae,\n",
        "    },\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\": False,\n",
        "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
        "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
        "        \"network_weights\": network_weight,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_args\": eval(network_args) if network_args else None,\n",
        "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
        "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
        "        \"training_comment\": None,\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\": optimizer_type,\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "        \"debug_dataset\": False,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\": output_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"save_precision\": save_precision,\n",
        "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
        "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
        "        \"save_last_n_epochs\": None,\n",
        "        \"save_state\": None,\n",
        "        \"save_last_n_epochs_state\": None,\n",
        "        \"resume\": None,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"max_token_length\": 225,\n",
        "        \"mem_eff_attn\": False,\n",
        "        \"xformers\": True,\n",
        "        \"max_train_epochs\": num_epochs,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\": gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"clip_skip\": clip_skip if not v2 else None,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
        "        \"lowram\": lowram,\n",
        "    },\n",
        "    \"sample_prompt_arguments\":{\n",
        "        \"sample_every_n_steps\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_steps\" else None,\n",
        "        \"sample_every_n_epochs\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_epochs\" else None,\n",
        "        \"sample_sampler\": sampler,\n",
        "    },\n",
        "    \"dreambooth_arguments\":{\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"saving_arguments\":{\n",
        "        \"save_model_as\": save_model_as\n",
        "    },\n",
        "}\n",
        "\n",
        "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "id": "-Z4w3lfFKLjr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.6. Start Training\n",
        "\n",
        "#@markdown Check your config here if you want to edit something: \n",
        "#@markdown - `sample_prompt` : /content/LoRA/config/sample_prompt.txt\n",
        "#@markdown - `config_file` : /content/LoRA/config/config_file.toml\n",
        "#@markdown - `dataset_config` : /content/LoRA/config/dataset_config.toml\n",
        "\n",
        "#@markdown Generated sample can be seen here: /content/LoRA/output/sample\n",
        "\n",
        "#@markdown You can import config from another session if you want.\n",
        "sample_prompt = \"/content/LoRA/config/sample_prompt.txt\" #@param {type:'string'}\n",
        "config_file = \"/content/LoRA/config/config_file.toml\" #@param {type:'string'}\n",
        "dataset_config = \"/content/LoRA/config/dataset_config.toml\" #@param {type:'string'}\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!accelerate launch \\\n",
        "  --config_file={accelerate_config} \\\n",
        "  --num_cpu_threads_per_process=1 \\\n",
        "  train_network.py \\\n",
        "  --sample_prompts={sample_prompt} \\\n",
        "  --dataset_config={dataset_config} \\\n",
        "  --config_file={config_file}\n",
        "\n"
      ],
      "metadata": {
        "id": "p_SHtbFwHVl1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. Testing"
      ],
      "metadata": {
        "id": "reMcN0bM_o53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.4. Visualize loss graph (Optional)\n",
        "training_logs_path = \"/content/LoRA/logs\" #@param {type : \"string\"}\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {training_logs_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TLSQslfFcQde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}